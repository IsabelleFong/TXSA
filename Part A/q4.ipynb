{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a130d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "from nltk.lm import MLE, Laplace\n",
    "from nltk.lm.preprocessing import everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d3bd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> He read a book </s>\n",
      "<s> I read a different book </s>\n",
      "<s> He read a book by Danielle </s>\n",
      "<s> I read a different book by Danielle </s>\n"
     ]
    }
   ],
   "source": [
    "# 1. Read the file\n",
    "file_path = 'Dataset/Data_3.txt'\n",
    "with open(file_path, 'r') as f:\n",
    "    # ONLY take lines that contain both <s> and </s>\n",
    "    lines = [line.strip() for line in f.readlines() if '<s>' in line and '</s>' in line]\n",
    "\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20245d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 21\n",
      "Unique Vocabulary: {'Danielle', 'read', 'I', 'book', 'a', 'different', 'by', '<s>', '</s>', 'He'}\n"
     ]
    }
   ],
   "source": [
    "# 2. Keep sentences as lists of words\n",
    "# We use all lines for training except the last line (last line is the target)\n",
    "tokenized_sentences = [line.split() for line in lines[:-1]]\n",
    "unique_words = set(word for sent in tokenized_sentences for word in sent)\n",
    "\n",
    "print(f\"Total Tokens: {sum(len(sent) for sent in tokenized_sentences)}\")\n",
    "print(f\"Unique Vocabulary: {unique_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b563fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "train_data = [list(everygrams(sent, max_len=n)) for sent in tokenized_sentences]\n",
    "\n",
    "# 3. Generate Vocab Data manually\n",
    "vocab_data = [word for sent in tokenized_sentences for word in sent]\n",
    "\n",
    "# 4. Initialize and Fit the model\n",
    "model = MLE(n)\n",
    "model.fit(train_data, vocab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8db85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Unigram Counts ---\n",
      "Count(<s>): 3\n",
      "Count(He): 2\n",
      "Count(read): 3\n",
      "Count(a): 3\n",
      "Count(book): 3\n",
      "Count(</s>): 3\n",
      "Count(I): 1\n",
      "Count(different): 1\n",
      "Count(by): 1\n",
      "Count(Danielle): 1\n",
      "Count(<UNK>): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Unigram Counts ---\")\n",
    "for word in model.vocab:\n",
    "    print(f\"Count({word}): {model.counts[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca5705b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bigram Counts for Corpus Sentences ---\n",
      "Count(He, read): 2\n",
      "Count(<s>, I): 1\n",
      "Count(different, book): 1\n",
      "Count(book, </s>): 2\n",
      "Count(by, Danielle): 1\n",
      "Count(a, different): 1\n",
      "Count(Danielle, </s>): 1\n",
      "Count(</s>, <s>): 0\n",
      "Count(I, read): 1\n",
      "Count(read, a): 3\n",
      "Count(book, by): 1\n",
      "Count(<s>, He): 2\n",
      "Count(a, book): 2\n"
     ]
    }
   ],
   "source": [
    "# Print specific bigram counts for the corpus sentences\n",
    "print(\"\\n--- Bigram Counts for Corpus Sentences ---\")\n",
    "for w1, w2 in set(bigrams([word for sent in tokenized_sentences for word in sent])):\n",
    "    # model.counts[[w1]][w2] retrieves the count of w2 following w1 \n",
    "    count = model.counts[[w1]][w2]\n",
    "    print(f\"Count({w1}, {w2}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b13b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-prepare generators for Laplace\n",
    "train_data_lap = [list(everygrams(sent, max_len=n)) for sent in tokenized_sentences]\n",
    "model_laplace = Laplace(n)\n",
    "model_laplace.fit(train_data_lap, vocab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c20dbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'He',\n",
       " 'read',\n",
       " 'a',\n",
       " 'book',\n",
       " '</s>',\n",
       " 'I',\n",
       " 'different',\n",
       " 'by',\n",
       " 'Danielle',\n",
       " '<UNK>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_laplace.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df03c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Sentence: <s> I read a different book by Danielle </s>\n",
      "Target Bigrams: [('<s>', 'I'), ('I', 'read'), ('read', 'a'), ('a', 'different'), ('different', 'book'), ('book', 'by'), ('by', 'Danielle'), ('Danielle', '</s>')]\n",
      "\n",
      "Unsmoothed\n",
      "P(I|<s>): Count=1, Denom=3 -> 0.3333\n",
      "P(read|I): Count=1, Denom=1 -> 1.0000\n",
      "P(a|read): Count=3, Denom=3 -> 1.0000\n",
      "P(different|a): Count=1, Denom=3 -> 0.3333\n",
      "P(book|different): Count=1, Denom=1 -> 1.0000\n",
      "P(by|book): Count=1, Denom=3 -> 0.3333\n",
      "P(Danielle|by): Count=1, Denom=1 -> 1.0000\n",
      "P(</s>|Danielle): Count=1, Denom=1 -> 1.0000\n",
      "\n",
      "Unsmoothed Probability: 0.03704\n",
      "\n",
      "Laplace Smooth \n",
      "P(I|<s>): Count=1+1, Denom=3+10 -> 0.1538\n",
      "P(read|I): Count=1+1, Denom=1+10 -> 0.1818\n",
      "P(a|read): Count=3+1, Denom=3+10 -> 0.3077\n",
      "P(different|a): Count=1+1, Denom=3+10 -> 0.1538\n",
      "P(book|different): Count=1+1, Denom=1+10 -> 0.1818\n",
      "P(by|book): Count=1+1, Denom=3+10 -> 0.1538\n",
      "P(Danielle|by): Count=1+1, Denom=1+10 -> 0.1818\n",
      "P(</s>|Danielle): Count=1+1, Denom=1+10 -> 0.1818\n",
      "\n",
      "Smoothed Probability: 0.0000012244\n"
     ]
    }
   ],
   "source": [
    "# 3. Calculate Sentence Probability\n",
    "target = lines[-1].strip().split()\n",
    "print(f\"Target Sentence: {' '.join(target)}\")\n",
    "target_bgs = list(bigrams(target))\n",
    "print(f\"Target Bigrams: {target_bgs}\\n\")\n",
    "\n",
    "def get_prob(model, bgs, smoothed=False):\n",
    "    p = 1.0\n",
    "    # Use the number of unique words excluding any <UNK>\n",
    "    # This should be 10 for your specific corpus\n",
    "    V_manual = 10 \n",
    "    \n",
    "    for w1, w2 in bgs:\n",
    "        count_bg = model.counts[[w1]][w2]\n",
    "        count_uni = model.counts[w1]\n",
    "        \n",
    "        if smoothed:\n",
    "            # Manually apply Laplace: (C(bg) + 1) / (C(unigram) + 10)\n",
    "            score = (count_bg + 1) / (count_uni + V_manual)\n",
    "            print(f\"P({w2}|{w1}): Count={count_bg}+1, Denom={count_uni}+{V_manual} -> {score:.4f}\")\n",
    "        else:\n",
    "            score = model.score(w2, [w1])\n",
    "            print(f\"P({w2}|{w1}): Count={count_bg}, Denom={count_uni} -> {score:.4f}\")\n",
    "        p *= score\n",
    "        \n",
    "    return p\n",
    "\n",
    "# Call the function\n",
    "print(\"Unsmoothed\")\n",
    "u_p = get_prob(model, target_bgs, smoothed=False)\n",
    "print(f\"\\nUnsmoothed Probability: {u_p:.5f}\")\n",
    "\n",
    "print(\"\\nLaplace Smooth \")\n",
    "s_p = get_prob(model_laplace, target_bgs, smoothed=True)\n",
    "print(f\"\\nSmoothed Probability: {s_p:.10f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
